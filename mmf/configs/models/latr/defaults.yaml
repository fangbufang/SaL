model_config:
  latr:
    lr_scale_frcn: 0.1
    lr_scale_text_t5: 0.1
    lr_scale_ocr_t5: 0.1
    lr_scale_mmt: 1.0  # no scaling
    text_t5_init_from_t5_base: true
    text_t5:
      d_model: 768
      num_heads: 12
      d_ff: 3072
      num_layers: 1
    ocr_t5:
      d_model: 768
      num_heads: 12
      d_ff: 3072
      num_layers: 1
    obj:
      mmt_in_dim: 2048
      dropout_prob: 0.1
    ocr:
      mmt_in_dim: 3002  # 300 (FastText) + 604 (PHOC) + 2048 (Faster R-CNN) + 50 (all zeros; legacy)
      dropout_prob: 0.1
    mmt:
      d_model: 768
      dropout_rate: 0.1
      # hidden_size: 768
      # num_layers: 4
    classifier:
      type: linear
      ocr_max_num: 300
      ocr_ptr_net:
        hidden_size: 768
        query_key_size: 768
      params: {}
    model_data_dir: ${env.data_dir}
    losses:
    - type: m4c_decoding_bce_with_mask
